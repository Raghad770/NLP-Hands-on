# NLP Hands-on with Transformers

Welcome to my **NLP Hands-on** repository! This repo contains practical Colab notebooks where I study Natural Language Processing (NLP) using Transformer models from Hugging Face. 

## About This Repository

In this repo, you'll find a collection of notebooks that cover various NLP tasks and techniques using state-of-the-art Transformer architectures. Each notebook is written with:

- Clear explanations of the underlying NLP concepts and functions
- Step-by-step code implementations using Hugging Face libraries
- Helpful visuals and diagrams to make the ideas easier to understand

This repository is designed to be a learning resource for anyone interested in NLP with Transformers, whether you are a beginner or looking to deepen your knowledge.

## Notebooks Included
- `KPE_ch5.ipynb` — Key Phrase Extraction techniques and experiments
- `FineTuning_DistilBert_on_EmotionsDS.ipynb` — Fine-tuning DistilBERT on emotion classification dataset
- `Train_NER_model_with_spacy_on_CONLL_data.ipynb` — Training a Named Entity Recognition (NER) model using spaCy on CoNLL dataset
- `Transformers_Anatomy.ipynb` — Detailed study of Transformer architecture components

## Getting Started

To run these notebooks, you will need:

- Python 3.7 or higher
- Google Colab or Jupyter Notebook 
- Key libraries: `transformers`, `datasets`, `torch`, `spacy`, etc.


## How to Use

- Explore the notebooks in sequence to build a solid understanding of NLP with Transformers.
- Read the explanations and comments carefully to grasp the concepts.
- Run the code cells to see the models and functions in action.
- Refer to the visuals included in the notebooks for better clarity.
